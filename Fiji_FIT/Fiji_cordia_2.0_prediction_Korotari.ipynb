{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using Sentinel-2 Data\n",
    "\n",
    "This example uses training data from the\n",
    "[Coast Train](https://github.com/nick-murray/coastTrain) dataset\n",
    "along with Sentinel-2 data to demonstrate how to use a\n",
    "machine learning classifier, in this case, Random Forest, to\n",
    "assign a class to each pixel.\n",
    "\n",
    "This notebook combines lessons from previous notebooks into\n",
    "a comprehensive worked example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "First we load the required Python libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "from dask.distributed import Client as DaskClient\n",
    "from odc.stac import load, configure_s3_access\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import folium\n",
    "import joblib\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import odc.geo.xr  # noqa: F401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study site configuration\n",
    "\n",
    "Here we establish the STAC catalog we're using as well as a\n",
    "spatial and temporal extent. This can be anywhere, but this location\n",
    "near Kuching was chosen due to the training data having several\n",
    "classes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC Catalog URL\n",
    "# catalog = \"https://stac.staging.digitalearthpacific.org\"\n",
    "catalog = \"https://earth-search.aws.element84.com/v1\"\n",
    "# Create a STAC Client\n",
    "client = Client.open(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>1.1. Define your area of interest. Find the coordinates of the bottom left and top right corners of your bounding box / area of interest. \n",
    "\n",
    "Use QGIS, Google Maps or another site to find the coordinates. Make sure to use at least 4 or 5 decimal places. Lat = latitude and lon = longitude. The min is in the bottom left and max is in the top right.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 - input your four coordinates here \n",
    "min_lat = -16.5297786\n",
    "min_lon = 179.4065010\n",
    "max_lat = -16.5253252\n",
    "max_lon = 179.4101265\n",
    "\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>1.2. Define the time period of interest. \n",
    "\n",
    "You can now define the time you are interested in. It is goood to put the format as \"year-month\"/\"year-month\". For example \"2022-06/2024-09\" to get all images covering June 2022 to September 2024. You can try to just choose the flowering months in one year but may have issues with cloud cover. Try different time periods and see what happes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 - input your datetime here - recommend at least 3 months and max 3 years. \n",
    "datetime = \"2024-01/2024-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dask cluster to improve data load time. Only run this once.\n",
    "dask_client = DaskClient(n_workers=1, threads_per_worker=16, memory_limit='16GB')\n",
    "\n",
    "# Configure S3 access. Cloud defaults is an optimisation, while requester pays is required for Landsat\n",
    "configure_s3_access(cloud_defaults=True, requester_pays=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Next up we gather training data. This could be any geospatial point dataset\n",
    "with a column that is numeric, for the class.\n",
    "\n",
    "If you'd like to explore the structure of this data, you can run `gdf.head()`\n",
    "to see the first few rows. The `explore()` function with the `column` argument\n",
    "will show the data on the map, and change the colour based on that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>2.1. Input your data from the field.  \n",
    "\n",
    "If you have new data, save your data as a geojson in QGIS and then drag and drop it into the same folder as this notebook in DEP. Then you will have to put the name of the file in the brackets below inside of quotes: eg. `'name.geojson'`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 - input your data file here inside the '' \n",
    "gdf = gpd.read_file('Training_Data/Korotari_clean1.geojson', bbox=bbox)\n",
    "\n",
    "# gdf = gdf.fillna(0)\n",
    "gdf.explore(column=\"Random_Forest\", legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and load Sentinel-2 data\n",
    "\n",
    "Here we search for Sentinel-2 scenes over our study area and use\n",
    "Dask to lazy-load them. We're only loading the red, green, blue, nir and swir\n",
    "bands, along with the scene classification (scl) band."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>2.2. - define the satellite image collections you wish to use: \n",
    "\n",
    "Input your collections name inside the brackets: \n",
    "\n",
    "You may try:   \n",
    "\"sentinel-2-c1-l2a\" (10mx10m resolution pixels)   \n",
    "OR   \n",
    "\"landsat-c2-l2\" (30mx30m resolution pixels)\n",
    "\n",
    "<font color='blue'>2.3. - define your cloud cover threshold. We recommend a number somewhere between 10-50. No need for quotes. \n",
    "\n",
    "Write this next to \"lt\": eg. {\"lt\": 50}} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data\n",
    "items = client.search(\n",
    "    collections=[\"sentinel-2-c1-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 40 }},\n",
    ").item_collection()\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into an xarray Dataset\n",
    "data = load(\n",
    "    items,\n",
    "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"scl\"],\n",
    "    bbox=bbox,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Now that we have data, we need to clean it up, masking out clouds\n",
    "and scaling values to between 0-1, which are the valid reflectance\n",
    "values.\n",
    "\n",
    "We add a couple of indices too, which will help the machine learning\n",
    "algorithm.\n",
    "\n",
    "Note that we still have a lazy-loaded array, and haven't transferred\n",
    "any data over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out clouds and scale values\n",
    "\n",
    "# Apply Sentinel-2 cloud mask\n",
    "# 1: defective, 3: shadow, 9: high confidence cloud, 10: thin cirrus\n",
    "mask_flags = [1, 3, 9, 10]\n",
    "\n",
    "cloud_mask = ~data.scl.isin(mask_flags)\n",
    "masked = data.where(cloud_mask)\n",
    "\n",
    "# Apply scaling and clip to valid data, from 0 to 1\n",
    "scaled = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "# Add some indices\n",
    "scaled[\"ndvi\"] = (scaled.nir08 - scaled.red) / (scaled.nir08 + scaled.red)\n",
    "# scaled[\"ndwi\"] = (scaled.green - scaled.nir08) / (scaled.green + scaled.nir08)\n",
    "\n",
    "scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise one date, to make sure it looks good.\n",
    "# This example shows empty areas where we've masked out nodata, but\n",
    "# note that there are still a lot of clouds coming in!\n",
    "\n",
    "scaled.isel(time=0).odc.explore(vmin=0, vmax=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cloud-free composite\n",
    "\n",
    "The final data preparation step involves creating a temporal\n",
    "median of the data bands. Here we use `compute()` to process\n",
    "the data and bring it into memory.\n",
    "\n",
    "We preview the data in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a median composite, which should get rid of most of the remaining clouds\n",
    "# Note that this will take a few minutes to complete\n",
    "\n",
    "median = scaled.median(\"time\").compute()\n",
    "\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>3.1. - visualise the resulting median satellite image:\n",
    "\n",
    "`median.odc.explore(vmin=0, vmax=0.3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median.odc.explore(vmin=0, vmax=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data array\n",
    "\n",
    "This next step involves extracting observed values from the satellite data\n",
    "and combining them with our point data, resulting in something like this:\n",
    "\n",
    "`class, red, green, blue ...`\n",
    "\n",
    "This structure is then fed into the machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First transform the training points to the same CRS as the data\n",
    "training = gdf.to_crs(median.odc.geobox.crs)\n",
    "\n",
    "# Next get the X and Y values out of the point geometries\n",
    "training_da = training.assign(x=training.geometry.x, y=training.geometry.y).to_xarray()\n",
    "\n",
    "# Now we can use the x and y values (lon, lat) to extract values from the median composite\n",
    "training_values = (\n",
    "    median.sel(training_da[[\"x\", \"y\"]], method=\"nearest\").squeeze().compute().to_pandas()\n",
    ")\n",
    "\n",
    "# Join the training data with the extracted values and remove unnecessary columns\n",
    "training_array = pd.concat([training[\"Random_Forest\"], training_values], axis=1)\n",
    "training_array = training_array.drop(\n",
    "    columns=[\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"spatial_ref\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop rows where there was no data available\n",
    "training_array = training_array.dropna()\n",
    "\n",
    "# Preview our resulting training array\n",
    "training_array.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classifier and fit a model\n",
    "\n",
    "We pass in simple numpy arrays to the classifier, one has the\n",
    "observations (the values of the red, green, blue and so on)\n",
    "while the other has the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are the first column\n",
    "classes = np.array(training_array)[:, 0]\n",
    "\n",
    "# The observation data is everything after the first column\n",
    "observations = np.array(training_array)[:, 1:]\n",
    "\n",
    "# Create a model...\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# ...and fit it to the data\n",
    "model = classifier.fit(observations, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Next we predict. Again, we need a simple numpy array, this time\n",
    "just with the observations. This needs to be in long array where\n",
    "the x dimension is the observation values and the y is each cell\n",
    "in the original raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a stacked array of observations\n",
    "stacked_arrays = median.to_array().stack(dims=[\"y\", \"x\"]).transpose()\n",
    "\n",
    "# Predict the classes\n",
    "predicted = model.predict(stacked_arrays)\n",
    "\n",
    "# Reshape back to the original 2D array\n",
    "array = predicted.reshape(len(median.y), len(median.x))\n",
    "\n",
    "# Convert to an xarray again, because it's easier to work with\n",
    "predicted_da = xr.DataArray(\n",
    "    array, coords={\"y\": masked.y, \"x\": masked.x}, dims=[\"y\", \"x\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise our results\n",
    "\n",
    "Here we're visualising the results along with the RGB image\n",
    "and the original training data points. We're doing this using\n",
    "a Python library called Folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_da.dtype)  # Check the dtype of your DataArray\n",
    "predicted_da = predicted_da.astype('float32')  # Convert to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a single interactive map\n",
    "# center = [np.mean([min_lat[0], max_lat[0]]), np.mean([min_lat[1], max_lat[1]])]\n",
    "# m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "center = [(min_lat + max_lat) / 2, (min_lon + max_lon) / 2]  # Assuming min_lon and max_lon are defined\n",
    "m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "\n",
    "\n",
    "# RGB for the median\n",
    "median.odc.to_rgba(vmin=0, vmax=0.3).odc.add_to(m, name=\"Median Composite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>4.1. - visualise the resulting machine learning prediction:\n",
    "\n",
    "The name of your model prediction is `predicted_da` so input this before `odc.add_to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical for the predicted classes and for the training data\n",
    "predicted_da.odc.add_to(m, name=\"Predicted\")\n",
    "gdf.explore(m=m, column=\"Random_Forest\", legend=True, name=\"Training Data\")\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained sklearn model (in your code earlier `model = classifier.fit(...)`)\n",
    "# ensure 'model' exists and observations/classes are defined as numpy arrays\n",
    "\n",
    "# If observations may contain NaNs, filter them out first:\n",
    "import numpy as np\n",
    "mask_valid = ~np.any(np.isnan(observations), axis=1)   # True for rows without NaN\n",
    "print(\"Valid training rows:\", mask_valid.sum(), \"out of\", len(mask_valid))\n",
    "\n",
    "train_preds = np.full(len(observations), np.nan)\n",
    "if mask_valid.sum() > 0:\n",
    "    train_preds[mask_valid] = model.predict(observations[mask_valid])\n",
    "\n",
    "# Report using only valid rows\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "valid_idx = mask_valid\n",
    "print(classification_report(classes[valid_idx], train_preds[valid_idx]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(classes[valid_idx], train_preds[valid_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Filter out rows with NaNs (same as before)\n",
    "# ------------------------------------------------------------------\n",
    "mask_valid = ~np.any(np.isnan(observations), axis=1)   # True for rows without NaN\n",
    "print(\"Valid rows (no NaN):\", mask_valid.sum(), \"out of\", len(mask_valid))\n",
    "\n",
    "X_valid = observations[mask_valid]\n",
    "y_valid = classes[mask_valid]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Remove classes that have only 1 sample (needed for stratify)\n",
    "# ------------------------------------------------------------------\n",
    "unique_classes, counts = np.unique(y_valid, return_counts=True)\n",
    "rare_classes = unique_classes[counts < 2]\n",
    "\n",
    "if len(rare_classes) > 0:\n",
    "    print(\"Removing rare classes with only 1 sample:\", rare_classes)\n",
    "\n",
    "    mask_enough = ~np.isin(y_valid, rare_classes)\n",
    "    X_valid = X_valid[mask_enough]\n",
    "    y_valid = y_valid[mask_enough]\n",
    "\n",
    "print(\"Samples remaining after removing rare classes:\", len(y_valid))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. 80/20 train–test split (stratified)\n",
    "# ------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    test_size=0.2,       # 20% test\n",
    "    random_state=42,     # for reproducibility\n",
    "    stratify=y_valid     # keep class proportions\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(y_train), \"Test size:\", len(y_test))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Train the model on the training set\n",
    "#    (assumes you already defined `classifier`, e.g. RandomForestClassifier(...))\n",
    "# ------------------------------------------------------------------\n",
    "model = classifier.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Predict on the test set\n",
    "# ------------------------------------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Metrics: classification report, confusion matrix,\n",
    "#    plus explicit precision / recall / F1\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Classification report (80/20 split):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Overall scores (change 'weighted' to 'macro' if you prefer)\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "recall    = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1        = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"Weighted precision: {precision:.3f}\")\n",
    "print(f\"Weighted recall:    {recall:.3f}\")\n",
    "print(f\"Weighted F1-score:  {f1:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Recompute just to keep it self-contained\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Match these labels to your actual class codes (sorted by row/column order)\n",
    "class_labels = np.unique(y_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "ax.set_title(\"Confusion Matrix (80/20 split)\")\n",
    "fig.colorbar(im)\n",
    "\n",
    "ax.set_xticks(np.arange(len(class_labels)))\n",
    "ax.set_yticks(np.arange(len(class_labels)))\n",
    "ax.set_xticklabels(class_labels)\n",
    "ax.set_yticklabels(class_labels)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "\n",
    "# Optional: add counts on each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1️⃣ Filter out NaN rows\n",
    "# ------------------------------------------------------------------\n",
    "mask_valid = ~np.any(np.isnan(observations), axis=1)\n",
    "X_valid = observations[mask_valid]\n",
    "y_valid = classes[mask_valid]\n",
    "print(\"Valid samples:\", len(y_valid))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2️⃣ Remove any classes with only 1 sample (stratify requirement)\n",
    "# ------------------------------------------------------------------\n",
    "unique_classes, counts = np.unique(y_valid, return_counts=True)\n",
    "rare_classes = unique_classes[counts < 2]\n",
    "if len(rare_classes) > 0:\n",
    "    print(\"Removing rare classes:\", rare_classes)\n",
    "    mask_enough = ~np.isin(y_valid, rare_classes)\n",
    "    X_valid = X_valid[mask_enough]\n",
    "    y_valid = y_valid[mask_enough]\n",
    "\n",
    "print(\"Samples used:\", len(y_valid))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3️⃣ Train/Test split 80/20\n",
    "# ------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_valid\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(y_train), \"Test:\", len(y_test))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4️⃣ Fit + Predict\n",
    "# ------------------------------------------------------------------\n",
    "model = classifier.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5️⃣ Print Classification Report + Precision / Recall / F1\n",
    "# ------------------------------------------------------------------\n",
    "print(\"=== Classification Report (80/20 Split) ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "recall    = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1        = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"Weighted Precision: {precision:.3f}\")\n",
    "print(f\"Weighted Recall:    {recall:.3f}\")\n",
    "print(f\"Weighted F1-score:  {f1:.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6️⃣ Confusion Matrix — NICE visual\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = np.unique(y_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "\n",
    "plt.title(\"Confusion Matrix (80/20 Split)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create normalized percentage confusion matrix\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"figure.dpi\": 300  # high resolution\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# ---------- COUNTS HEATMAP ----------\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Spectral_r\",\n",
    "    linewidths=0.7,\n",
    "    linecolor=\"black\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    ax=axes[0],\n",
    "    cbar=False\n",
    ")\n",
    "axes[0].set_title(\"Confusion Matrix (Counts)\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "\n",
    "# ---------- NORMALIZED PERCENT HEATMAP ----------\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"turbo\",\n",
    "    linewidths=0.7,\n",
    "    linecolor=\"black\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    ax=axes[1],\n",
    "    cbar=True,\n",
    "    cbar_kws={\"shrink\": 0.7, \"label\": \"% of True Class\"}\n",
    ")\n",
    "axes[1].set_title(\"Confusion Matrix (%)\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Model Performance – 80/20 Split\", fontsize=18, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save for paper (PNG high-res)\n",
    "plt.savefig(\"confusion_matrix_paper_ready.png\",\n",
    "            dpi=400, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Get classification report as a dictionary\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Remove summary rows if you want only class metrics\n",
    "df_classes = df_report.drop([\"accuracy\", \"macro avg\", \"weighted avg\"], errors=\"ignore\")\n",
    "\n",
    "# Only keep useful columns\n",
    "df_classes = df_classes[[\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    df_classes,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"viridis\",   # <-- change colormap here if desired\n",
    "    linewidths=0.7,\n",
    "    linecolor=\"black\",\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "plt.title(\"Classification Report (Color-coded Table)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Metrics\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Class Label\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high resolution version for paper use\n",
    "plt.savefig(\"classification_report_colortable.png\", dpi=400, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Your class name mapping\n",
    "class_names = {\n",
    "    1.0: \"African_Tulip\",\n",
    "    3.0: \"Breadfruit_Tree\",\n",
    "    4.0: \"Cocoa_Tree\",\n",
    "    5.0: \"Cordia_Salmwood\",\n",
    "    7.0: \"Guava_Tree\",\n",
    "    8.0: \"Merremia_Peltata\",\n",
    "    9.0: \"Native_Forest\",\n",
    "    10.0: \"Raintree\",\n",
    "    13.0: \"Shrubs\"\n",
    "}\n",
    "\n",
    "# Convert labels into species names in the same order as cm rows/columns\n",
    "label_names = [class_names[l] for l in labels]\n",
    "\n",
    "# Create normalized percentage confusion matrix\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"figure.dpi\": 300\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# ---------- COUNTS HEATMAP ----------\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Spectral_r\",\n",
    "    linewidths=0.7,\n",
    "    linecolor=\"black\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    ax=axes[0],\n",
    "    cbar=False\n",
    ")\n",
    "axes[0].set_title(\"Confusion Matrix (Counts)\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Predicted Species\", fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"True Species\", fontweight=\"bold\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ---------- NORMALIZED PERCENT HEATMAP ----------\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"turbo\",\n",
    "    linewidths=0.7,\n",
    "    linecolor=\"black\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    ax=axes[1],\n",
    "    cbar=True,\n",
    "    cbar_kws={\"shrink\": 0.7, \"label\": \"% of True Class\"}\n",
    ")\n",
    "axes[1].set_title(\"Confusion Matrix (%)\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Predicted Species\", fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle(\"Model Performance – 80/20 Split (Species-Level Confusion Matrix)\", fontsize=18, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"confusion_matrix_with_species.png\", dpi=400, bbox_inches=\"tight\", transparent=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Your mapping of class numbers to species names ---\n",
    "class_names = {\n",
    "    1.0: \"African_Tulip\",\n",
    "    3.0: \"Breadfruit_Tree\",\n",
    "    4.0: \"Cocoa_Tree\",\n",
    "    5.0: \"Cordia_Salmwood\",\n",
    "    7.0: \"Guava_Tree\",\n",
    "    8.0: \"Merremia_Peltata\",\n",
    "    9.0: \"Native_Forest\",\n",
    "    10.0: \"Raintree\",\n",
    "    13.0: \"Shrubs\"\n",
    "}\n",
    "\n",
    "# --- Step 1: Get classification report dictionary ---\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "# --- Step 2: Remove summary rows (accuracy, macro avg, weighted avg) ---\n",
    "df_classes = df_report.drop([\"accuracy\", \"macro avg\", \"weighted avg\"], errors=\"ignore\")\n",
    "\n",
    "# --- Step 3: Replace numeric class labels with species names ---\n",
    "df_classes.index = [\n",
    "    class_names[float(i)] if i not in [\"accuracy\", \"macro avg\", \"weighted avg\"] else i\n",
    "    for i in df_classes.index\n",
    "]\n",
    "\n",
    "# --- Step 4: Keep essential metrics ---\n",
    "df_classes = df_classes[[\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
    "\n",
    "# --- Step 5: Create heatmap ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(\n",
    "    df_classes,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"viridis\",\n",
    "    linewidths=0.7,\n",
    "    linecolor=\"black\",\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "plt.title(\"Classification Report (Species-Level, Color-coded Table)\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Metrics\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Species\", fontweight=\"bold\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Step 6: Save high-res figure ---\n",
    "plt.savefig(\"classification_report_species_colortable.png\", dpi=400, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>4.2. - write this model output map to a new file:\n",
    "\n",
    "Input the name of your model prediction before `.odc.write_cog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_da.odc.write_cog(\"Korotari_v2.tif\", overwrite=True)\n",
    "# predicted_da.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of the 'Random_Forest' column\n",
    "print(gdf['Random_Forest'].dtype)\n",
    "\n",
    "# Display the first few rows of the 'Random_Forest' column to inspect its contents\n",
    "print(gdf['Random_Forest'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the column contains text data\n",
    "african_tulip_count = gdf[gdf['Random_Forest'] == '6'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming '2' represents African Tulip in the Random_Forest column\n",
    "\n",
    "# Total number of predictions\n",
    "total_predictions = gdf['Random_Forest'].count()\n",
    "\n",
    "# Number of African Tulip predictions (where Random_Forest equals 2)\n",
    "african_tulip_count = gdf[gdf['Random_Forest'] == 2].shape[0]\n",
    "\n",
    "# Calculate the percentage of African Tulip\n",
    "percentage_african_tulip = (african_tulip_count / total_predictions) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of Settlements: {percentage_african_tulip:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_percentage = gdf[\"Random_Forest\"].value_counts(normalize=True) * 100\n",
    "print(rf_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>5.1. - download the file, load, explore and set new colours in QGIS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"cordia_v.2.0.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "Do the results make sense?\n",
    "\n",
    "What are some of the limitations of the visualisation?\n",
    "\n",
    "### Next steps\n",
    "\n",
    "The obvious next step is to fine tune the data. Perhaps download the points for this\n",
    "region of interest as well as the RGB image and add and remove points until\n",
    "there is a more representative training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New AOI for your interest\n",
    "\n",
    "Choose a new AoI and ToI based on your interests. This could be for wider Tongatapu, Eua or even Vava'u.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = -17.772398\n",
    "min_lon = 178.401739\n",
    "max_lat = -17.766376\n",
    "max_lon = 178.407854\n",
    "\n",
    "bbox = [min_lon, min_lat, max_lon, max_lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 - input your four coordinates here \n",
    "min_lat = -17.772421\n",
    "min_lon = 178.37\n",
    "max_lat = -17.766539\n",
    "max_lon = 178.401809\n",
    "\n",
    "bbox_wainibuka = [min_lon, min_lat, max_lon, max_lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data\n",
    "items = client.search(\n",
    "    collections=[\"sentinel-2-c1-l2a\"],\n",
    "    bbox=bbox_wainibuka,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 15 }},\n",
    ").item_collection()\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into an xarray Dataset\n",
    "data_Wainibuka = load(\n",
    "    items,\n",
    "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"scl\"],\n",
    "    bbox=bbox_wainibuka,\n",
    "    chunks={\"x\": 800, \"y\": 800},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "\n",
    "# data_Eua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out clouds and scale values\n",
    "\n",
    "# Apply Sentinel-2 cloud mask\n",
    "# 1: defective, 3: shadow, 9: high confidence cloud, 10: thin cirrus\n",
    "mask_flags = [1, 3, 9, 10]\n",
    "\n",
    "cloud_mask = ~data_Wainibuka.scl.isin(mask_flags)\n",
    "masked = data_Wainibuka.where(cloud_mask)\n",
    "\n",
    "# Apply scaling and clip to valid data, from 0 to 1\n",
    "scaled_Wainibuka = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "# Add some indices\n",
    "scaled_Wainibuka[\"ndvi\"] = (scaled_Wainibuka.nir08 - scaled_Wainibuka.red) / (scaled_Wainibuka.nir08 + scaled_Wainibuka.red)\n",
    "# scaled[\"ndwi\"] = (scaled.green - scaled.nir08) / (scaled.green + scaled.nir08)\n",
    "\n",
    "# Create a median composite, which should get rid of most of the remaining clouds\n",
    "# Note that this will take a few minutes to complete\n",
    "\n",
    "median_Wainibuka = scaled_Wainibuka.median(\"time\").compute()\n",
    "\n",
    "# median\n",
    "\n",
    "median_Wainibuka.odc.explore(vmin=0, vmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a stacked array of observations\n",
    "stacked_arrays = median_Wainibuka.to_array().stack(dims=[\"y\", \"x\"]).transpose()\n",
    "\n",
    "# Predict the classes\n",
    "predicted_Wainibuka = model.predict(stacked_arrays)\n",
    "\n",
    "# Reshape back to the original 2D array\n",
    "array = predicted_Wainibuka.reshape(len(median_Wainibuka.y), len(median_Wainibuka.x))\n",
    "\n",
    "# Convert to an xarray again, because it's easier to work with\n",
    "predicted_da = xr.DataArray(\n",
    "    array, coords={\"y\": masked.y, \"x\": masked.x}, dims=[\"y\", \"x\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_da.dtype)  # Check the dtype of your DataArray\n",
    "predicted_da = predicted_da.astype('float32')  # Convert to float32\n",
    "\n",
    "# Put it all on a single interactive map\n",
    "# center = [np.mean([min_lat[0], max_lat[0]]), np.mean([min_lat[1], max_lat[1]])]\n",
    "# m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "center = [(min_lat + max_lat) / 2, (min_lon + max_lon) / 2]  # Assuming min_lon and max_lon are defined\n",
    "m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "# RGB for the median\n",
    "median_Wainibuka.odc.to_rgba(vmin=0, vmax=0.3).odc.add_to(m, name=\"Median Composite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical for the predicted classes and for the training data\n",
    "predicted_da.odc.add_to(m, name=\"Predicted\")\n",
    "gdf.explore(m=m, column=\"Random_Forest\", legend=True, name=\"Training Data\")\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_da.odc.write_cog(\"AT_Wainibuka_Jan_Oct_2024.tif\", overwrite=True)\n",
    "# predicted_da.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
