{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, configuration, and helpers\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.ndimage import binary_dilation, binary_closing\n",
    "import joblib\n",
    "import folium\n",
    "\n",
    "# optional rioxarray for GeoTIFF saving\n",
    "try:\n",
    "    import rioxarray  # noqa: F401\n",
    "    RIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    RIO_AVAILABLE = False\n",
    "\n",
    "# User configuration (edit)\n",
    "STAC_CATALOG = \"https://earth-search.aws.element84.com/v1\"\n",
    "AOI_PATH = \"AOI/EfateAOI.geojson\"            # set to your local path or URL\n",
    "TRAINING_PATH = \"Training_Data/InvasiveClean6.geojson\"\n",
    "DATETIME = \"2024-05/2024-09\"\n",
    "SENTINEL_COLLECTION = [\"sentinel-2-c1-l2a\"]\n",
    "\n",
    "MASK_NAME = \"combined_mask\"\n",
    "INCLUDE_MASK_AS_FEATURE = False   # True to include mask as feature, False to only exclude pixels\n",
    "RF_N_ESTIMATORS = 100\n",
    "RF_RANDOM_STATE = 42\n",
    "MODEL_OUT = \"rf_model.joblib\"\n",
    "PRED_GTIFF = \"predicted.tif\"\n",
    "OUT_DS_NETCDF = \"predicted_dataset.nc\"\n",
    "\n",
    "def info(*args):\n",
    "    print(\"[INFO]\", *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC client, Dask, and robust AOI loading\n",
    "from pystac_client import Client\n",
    "from dask.distributed import Client as DaskClient\n",
    "from odc.stac import load, configure_s3_access\n",
    "\n",
    "# Start dask client (adjust to your environment)\n",
    "dask_client = DaskClient(n_workers=1, threads_per_worker=8, memory_limit=\"16GB\")\n",
    "info(\"Started Dask client:\", dask_client)\n",
    "\n",
    "# configure s3 access (ODC)\n",
    "configure_s3_access(cloud_defaults=True, requester_pays=True)\n",
    "info(\"Configured S3 access\")\n",
    "\n",
    "# Robust AOI load: try path(s), else ask user or create small bbox fallback\n",
    "def load_aoi(aoi_path):\n",
    "    p = Path(aoi_path)\n",
    "    if p.exists():\n",
    "        return gpd.read_file(str(p))\n",
    "    # try URL fallback if path looks like URL\n",
    "    try:\n",
    "        if aoi_path.startswith(\"http\"):\n",
    "            return gpd.read_file(aoi_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: prompt or create a simple bbox AOI (edit coords as needed)\n",
    "    info(\"AOI not found at\", aoi_path)\n",
    "    # Example fallback bbox (replace with your coords if desired)\n",
    "    minx, miny, maxx, maxy = 168.2, -17.6, 168.7, -17.1\n",
    "    from shapely.geometry import box\n",
    "    aoi = gpd.GeoDataFrame({\"id\":[1]}, geometry=[box(minx, miny, maxx, maxy)], crs=\"EPSG:4326\")\n",
    "    info(\"Using fallback bbox AOI\")\n",
    "    return aoi\n",
    "\n",
    "aoi_gdf = load_aoi(AOI_PATH)\n",
    "info(\"AOI CRS:\", aoi_gdf.crs)\n",
    "aoi_gdf.plot(edgecolor=\"red\", facecolor=\"none\")\n",
    "plt.title(\"AOI check\")\n",
    "plt.show()\n",
    "bbox = aoi_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "info(\"BBox:\", bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training points\n",
    "if Path(TRAINING_PATH).exists() or TRAINING_PATH.startswith(\"http\"):\n",
    "    gdf = gpd.read_file(TRAINING_PATH, bbox=tuple(bbox))\n",
    "    info(\"Loaded training points:\", len(gdf))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Training file not found: {TRAINING_PATH}\")\n",
    "\n",
    "# quick interactive check\n",
    "try:\n",
    "    gdf.explore(column=\"randomforest\", legend=True)\n",
    "except Exception:\n",
    "    info(\"gdf.explore not available; here's head():\")\n",
    "    print(gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC search and load Sentinel-2 L2A bands via ODC\n",
    "client = Client.open(STAC_CATALOG)\n",
    "items = client.search(\n",
    "    collections=SENTINEL_COLLECTION,\n",
    "    bbox=bbox,\n",
    "    datetime=DATETIME,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 25}},\n",
    ").item_collection()\n",
    "info(\"Found STAC items:\", len(items))\n",
    "\n",
    "data = load(\n",
    "    items,\n",
    "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"scl\"],\n",
    "    bbox=bbox,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "info(\"Loaded data variables:\", list(data.data_vars))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud masking, scaling, indices, and median composite\n",
    "mask_flags = [1, 3, 9, 10]  # SCL values to remove\n",
    "cloud_mask = ~data.scl.isin(mask_flags)\n",
    "masked = data.where(cloud_mask)\n",
    "\n",
    "# scale to 0-1 and clip\n",
    "scaled = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "# add NDVI for reference\n",
    "scaled[\"ndvi\"] = (scaled.nir08 - scaled.red) / (scaled.nir08 + scaled.red)\n",
    "\n",
    "info(\"Computing median composite (this may take a while)...\")\n",
    "median = scaled.median(\"time\").compute()\n",
    "info(\"Median computed. Variables:\", list(median.data_vars))\n",
    "\n",
    "# quick visual check\n",
    "try:\n",
    "    median.odc.explore(vmin=0, vmax=0.3)\n",
    "except Exception:\n",
    "    info(\"median.odc.explore unavailable in this environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute indices, auto/select NDBI threshold, morphological expansion\n",
    "green = median[\"green\"]\n",
    "red = median[\"red\"]\n",
    "nir = median[\"nir08\"]\n",
    "swir = median[\"swir16\"]\n",
    "\n",
    "ndwi  = (green - nir) / (green + nir)\n",
    "ndbi  = (swir - nir) / (swir + nir)\n",
    "ndbai = (swir - red) / (swir + red)\n",
    "\n",
    "# Describe NDBI to choose threshold\n",
    "ndbi_vals = ndbi.values.flatten()\n",
    "ndbi_vals = ndbi_vals[np.isfinite(ndbi_vals)]\n",
    "info(\"NDBI percentiles (0,5,50,95,100):\", np.nanpercentile(ndbi_vals, [0,5,50,95,100]))\n",
    "\n",
    "# Automatic threshold with Otsu if enough data, fallback to default\n",
    "ndbi_thr = 0.1\n",
    "try:\n",
    "    if len(ndbi_vals) > 2000:\n",
    "        ndbi_thr = float(threshold_otsu(ndbi_vals))\n",
    "        info(\"Otsu selected NDBI threshold:\", ndbi_thr)\n",
    "    else:\n",
    "        info(\"Too few samples for Otsu; using default\", ndbi_thr)\n",
    "except Exception as e:\n",
    "    info(\"Otsu failed, using default NDBI threshold\", ndbi_thr, \"; reason:\", e)\n",
    "\n",
    "ndwi_thr = 0.2\n",
    "ndbai_thr = 0.15\n",
    "\n",
    "water_mask = (ndwi > ndwi_thr)\n",
    "building_mask = (ndbi > ndbi_thr)\n",
    "road_mask = (ndbai > ndbai_thr) & (ndwi < 0) & (ndbi < 0.2)\n",
    "\n",
    "# expand building mask a little to capture settlement edges\n",
    "y_dim, x_dim = ndwi.dims[-2], ndwi.dims[-1]\n",
    "bm_np = building_mask.values.astype(bool)\n",
    "bm_np = binary_dilation(bm_np, structure=np.ones((3,3)), iterations=1)\n",
    "bm_np = binary_closing(bm_np, structure=np.ones((3,3)), iterations=1)\n",
    "building_mask_expanded = xr.DataArray(bm_np, coords=[median[y_dim], median[x_dim]], dims=(y_dim, x_dim))\n",
    "\n",
    "combined_mask_da = (water_mask | building_mask_expanded | road_mask).astype(\"uint8\")\n",
    "combined_mask_da.name = MASK_NAME\n",
    "info(\"Combined mask counts (value:count):\", np.unique(combined_mask_da.values, return_counts=True))\n",
    "\n",
    "# quick view\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.title(\"combined_mask (0=keep,1=masked)\"); plt.imshow(combined_mask_da.values, cmap=\"gray\"); plt.axis(\"off\")\n",
    "plt.subplot(1,2,2); plt.title(\"ndbi histogram\"); plt.hist(ndbi_vals, bins=120); plt.axvline(ndbi_thr, color='r'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include mask as a feature or keep it for filtering\n",
    "if INCLUDE_MASK_AS_FEATURE:\n",
    "    median = median.assign(**{MASK_NAME: combined_mask_da.astype(\"int8\")})\n",
    "    info(\"Mask added as band to median; variables now:\", list(median.data_vars))\n",
    "else:\n",
    "    mask_da = combined_mask_da\n",
    "    info(\"Mask will be used to exclude pixels but not included as feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample median bands at training points, filter by mask, drop NaNs\n",
    "gdf_pts = gdf.to_crs(median.odc.geobox.crs)\n",
    "gx = gdf_pts.geometry.x.values\n",
    "gy = gdf_pts.geometry.y.values\n",
    "info(\"Training points reprojected to raster CRS:\", median.odc.geobox.crs)\n",
    "\n",
    "arr = median.to_array()  # dims: variable, y, x (inspect if different)\n",
    "info(\"arr dims:\", arr.dims)\n",
    "y_dim, x_dim = arr.dims[-2], arr.dims[-1]\n",
    "points_dim = \"points\"\n",
    "\n",
    "# Vectorized sampling using DataArray indexers\n",
    "gx_da = xr.DataArray(gx, dims=points_dim)\n",
    "gy_da = xr.DataArray(gy, dims=points_dim)\n",
    "sampled = arr.sel({y_dim: gy_da, x_dim: gx_da}, method=\"nearest\")\n",
    "info(\"sampled dims after sel:\", sampled.dims)\n",
    "\n",
    "# squeeze singleton dims if present\n",
    "sampled = sampled.squeeze(drop=True)\n",
    "info(\"sampled dims after squeeze:\", sampled.dims, \"ndim:\", sampled.ndim)\n",
    "\n",
    "# If still not 2D, fallback to explicit nearest-index lookup\n",
    "if sampled.ndim == 2:\n",
    "    sampled_df = sampled.transpose(points_dim, \"variable\").to_pandas()\n",
    "    sampled_df = pd.DataFrame(sampled_df).reset_index(drop=True)\n",
    "else:\n",
    "    info(\"Vectorized sampling produced ndim != 2; falling back to explicit nearest-index sampling (slower).\")\n",
    "    y_coords = arr.coords[y_dim].values\n",
    "    x_coords = arr.coords[x_dim].values\n",
    "    iy = np.array([np.abs(y_coords - yy).argmin() for yy in gy])\n",
    "    ix = np.array([np.abs(x_coords - xx).argmin() for xx in gx])\n",
    "    vals = np.stack([arr.isel({y_dim: iy_k, x_dim: ix_k}).values.ravel() for iy_k, ix_k in zip(iy, ix)], axis=0)\n",
    "    var_names = list(arr.coords[\"variable\"].values)\n",
    "    sampled_df = pd.DataFrame(vals, columns=var_names).reset_index(drop=True)\n",
    "\n",
    "# Combine labels + features\n",
    "training_df = pd.concat([gdf_pts.reset_index(drop=True)[\"randomforest\"], sampled_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Filter out training points that fall in masked areas (if not including mask as a feature)\n",
    "if not INCLUDE_MASK_AS_FEATURE:\n",
    "    mask_at_pts = combined_mask_da.sel({y_dim: gy, x_dim: gx}, method=\"nearest\").values\n",
    "    keep = (mask_at_pts == 0)\n",
    "    training_df = training_df.loc[keep].reset_index(drop=True)\n",
    "    info(f\"Kept {training_df.shape[0]} training points after mask filtering (out of {len(gx)})\")\n",
    "\n",
    "# Drop NaNs\n",
    "training_df = training_df.dropna()\n",
    "info(\"Training rows after dropna:\", len(training_df))\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest\n",
    "classes = training_df.iloc[:, 0].values\n",
    "observations = training_df.iloc[:, 1:].values\n",
    "info(\"Observations shape:\", observations.shape)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=RF_N_ESTIMATORS, random_state=RF_RANDOM_STATE, n_jobs=-1)\n",
    "clf.fit(observations, classes)\n",
    "info(\"RandomForest trained\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(clf, MODEL_OUT)\n",
    "info(\"Saved model to\", MODEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full image stack and predict\n",
    "arr = median.to_array()  # variable, y, x\n",
    "y_dim, x_dim = arr.dims[-2], arr.dims[-1]\n",
    "ny, nx = arr.sizes[y_dim], arr.sizes[x_dim]\n",
    "stacked = arr.stack(pixels=(y_dim, x_dim)).transpose(\"pixels\", \"variable\")  # pixels x variable\n",
    "X = stacked.values\n",
    "info(\"Full predictor array shape:\", X.shape)\n",
    "\n",
    "# valid data: no NaNs\n",
    "valid_data_mask = ~np.any(np.isnan(X), axis=1)\n",
    "\n",
    "if not INCLUDE_MASK_AS_FEATURE:\n",
    "    mask_flat = combined_mask_da.stack(pixels=(y_dim, x_dim)).values.astype(bool)\n",
    "    predict_mask = valid_data_mask & (~mask_flat)\n",
    "else:\n",
    "    predict_mask = valid_data_mask\n",
    "\n",
    "info(\"Pixels to predict:\", int(predict_mask.sum()), \"out of\", X.shape[0])\n",
    "\n",
    "pred_flat = np.full(X.shape[0], np.nan, dtype=np.float32)\n",
    "if predict_mask.sum() > 0:\n",
    "    pred_flat[predict_mask] = clf.predict(X[predict_mask]).astype(np.float32)\n",
    "\n",
    "pred_2d = pred_flat.reshape(ny, nx)\n",
    "predicted_da = xr.DataArray(pred_2d, coords={y_dim: median[y_dim], x_dim: median[x_dim]}, dims=(y_dim, x_dim))\n",
    "predicted_da.name = \"predicted_class\"\n",
    "info(\"Prediction finished, dtype:\", predicted_da.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the predicted result for visualization (so water/settlements/roads removed)\n",
    "predicted_masked = predicted_da.where(~combined_mask_da.astype(bool), other=np.nan)\n",
    "predicted_masked.name = \"predicted_masked\"\n",
    "info(\"Applied combined mask to predicted results.\")\n",
    "\n",
    "# Optionally build an output dataset including predictions + mask\n",
    "out_ds = xr.Dataset({\"predicted\": predicted_da, \"predicted_masked\": predicted_masked, MASK_NAME: combined_mask_da})\n",
    "info(\"Output dataset prepared with variables:\", list(out_ds.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs\n",
    "if RIO_AVAILABLE:\n",
    "    try:\n",
    "        out_ds[\"predicted\"].rio.write_crs(median.odc.geobox.crs, inplace=True)\n",
    "        out_ds[\"predicted\"].rio.to_raster(PRED_GTIFF)\n",
    "        info(\"Saved predicted GeoTIFF to\", PRED_GTIFF)\n",
    "    except Exception as e:\n",
    "        info(\"Could not save GeoTIFF:\", e)\n",
    "        out_ds.to_netcdf(OUT_DS_NETCDF)\n",
    "        info(\"Saved NetCDF to\", OUT_DS_NETCDF)\n",
    "else:\n",
    "    out_ds.to_netcdf(OUT_DS_NETCDF)\n",
    "    info(\"rioxarray not available; saved NetCDF to\", OUT_DS_NETCDF)\n",
    "\n",
    "# Save model already done in Cell 9; if you'd like to save the dataset to netCDF in addition:\n",
    "# out_ds.to_netcdf(\"out_with_mask.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive map (folium)\n",
    "# compute center in WGS84\n",
    "try:\n",
    "    gdf_wgs84 = gdf_pts.to_crs(epsg=4326)\n",
    "    center = [float(gdf_wgs84.geometry.y.mean()), float(gdf_wgs84.geometry.x.mean())]\n",
    "except Exception:\n",
    "    # fallback: use AOI centroid in WGS84\n",
    "    center = [float(aoi_gdf.to_crs(epsg=4326).geometry.centroid.y.mean()), float(aoi_gdf.to_crs(epsg=4326).geometry.centroid.x.mean())]\n",
    "\n",
    "m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "# Try odc helpers if available; otherwise user may export tiles or open the GeoTIFF in GIS\n",
    "try:\n",
    "    median.odc.to_rgba(vmin=0, vmax=0.3).odc.add_to(m, name=\"Median Composite\")\n",
    "except Exception:\n",
    "    info(\"ODC RGBA helper not available for median layer\")\n",
    "\n",
    "try:\n",
    "    predicted_masked.odc.add_to(m, name=\"Predicted (masked)\")\n",
    "except Exception:\n",
    "    info(\"ODC helper not available for predicted layer; consider exporting GeoTIFF and adding as tile layer\")\n",
    "\n",
    "# mask overlay (semi-transparent red for masked areas)\n",
    "try:\n",
    "    combined_mask_da.astype(\"uint8\").odc.to_rgba(palette=[\"none\", \"red\"], alpha=0.3).odc.add_to(m, name=\"Combined Mask\")\n",
    "except Exception:\n",
    "    info(\"ODC helper not available for mask display\")\n",
    "\n",
    "# Add training points (converted to WGS84)\n",
    "try:\n",
    "    gdf_wgs84.explore(m=m, column=\"randomforest\", legend=True, name=\"Training Data\")\n",
    "except Exception:\n",
    "    info(\"Could not add training points to map via .explore()\")\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A: Quick checks: are predictions present and how many NaNs after masking?\n",
    "import numpy as np\n",
    "\n",
    "# Use whichever predicted variable you have: predicted_da (unmasked) and predicted_masked (masked)\n",
    "if \"predicted_da\" not in globals():\n",
    "    print(\"predicted_da not found. Do you have `predicted_da` variable? (run the prediction cell first)\")\n",
    "else:\n",
    "    print(\"predicted_da dtype:\", predicted_da.dtype)\n",
    "    vals, counts = np.unique(np.nan_to_num(predicted_da.values, nan=-9999), return_counts=True)\n",
    "    print(\"Unique values (nan replaced by -9999):\")\n",
    "    for v,c in zip(vals,counts):\n",
    "        print(f\"  {v}: {c}\")\n",
    "\n",
    "if \"predicted_masked\" in globals():\n",
    "    vals_m, counts_m = np.unique(np.nan_to_num(predicted_masked.values, nan=-9999), return_counts=True)\n",
    "    print(\"\\npredicted_masked unique values (nan->-9999):\")\n",
    "    for v,c in zip(vals_m,counts_m):\n",
    "        print(f\"  {v}: {c}\")\n",
    "    # count masked pixels specifically\n",
    "    n_masked = np.sum(np.isnan(predicted_masked.values))\n",
    "    print(f\"\\nMasked pixels in predicted_masked (NaN): {int(n_masked)}\")\n",
    "else:\n",
    "    print(\"predicted_masked not present yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A: Quick checks: are predictions present and how many NaNs after masking?\n",
    "import numpy as np\n",
    "\n",
    "# Use whichever predicted variable you have: predicted_da (unmasked) and predicted_masked (masked)\n",
    "if \"predicted_da\" not in globals():\n",
    "    print(\"predicted_da not found. Do you have `predicted_da` variable? (run the prediction cell first)\")\n",
    "else:\n",
    "    print(\"predicted_da dtype:\", predicted_da.dtype)\n",
    "    vals, counts = np.unique(np.nan_to_num(predicted_da.values, nan=-9999), return_counts=True)\n",
    "    print(\"Unique values (nan replaced by -9999):\")\n",
    "    for v,c in zip(vals,counts):\n",
    "        print(f\"  {v}: {c}\")\n",
    "\n",
    "if \"predicted_masked\" in globals():\n",
    "    vals_m, counts_m = np.unique(np.nan_to_num(predicted_masked.values, nan=-9999), return_counts=True)\n",
    "    print(\"\\npredicted_masked unique values (nan->-9999):\")\n",
    "    for v,c in zip(vals_m,counts_m):\n",
    "        print(f\"  {v}: {c}\")\n",
    "    # count masked pixels specifically\n",
    "    n_masked = np.sum(np.isnan(predicted_masked.values))\n",
    "    print(f\"\\nMasked pixels in predicted_masked (NaN): {int(n_masked)}\")\n",
    "else:\n",
    "    print(\"predicted_masked not present yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C: Quick static visual overlay of RGB, predicted, and mask (matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# build an RGB with median bands (if available)\n",
    "if all(k in median for k in (\"red\",\"green\",\"blue\")):\n",
    "    r = median[\"red\"].values\n",
    "    g = median[\"green\"].values\n",
    "    b = median[\"blue\"].values\n",
    "    # If bands have shape (y,x) use directly; ensure 0-1\n",
    "    rgb = np.dstack([r, g, b])\n",
    "    # normalize for display if not already 0-1\n",
    "    rgb_display = np.clip(rgb, 0, 1)\n",
    "else:\n",
    "    rgb_display = None\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "if rgb_display is not None:\n",
    "    axs[0].imshow(rgb_display)\n",
    "    axs[0].set_title(\"RGB median\")\n",
    "else:\n",
    "    axs[0].text(0.5,0.5,\"RGB not available\", ha=\"center\")\n",
    "    axs[0].set_title(\"RGB median\")\n",
    "\n",
    "# predicted (raw)\n",
    "if \"predicted_da\" in globals():\n",
    "    im = axs[1].imshow(np.nan_to_num(predicted_da.values, nan=-1), cmap=\"viridis\")\n",
    "    axs[1].set_title(\"predicted_da (numeric classes)\")\n",
    "    plt.colorbar(im, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "else:\n",
    "    axs[1].text(0.5,0.5,\"predicted_da missing\", ha=\"center\")\n",
    "\n",
    "# mask\n",
    "if \"combined_mask_da\" in globals():\n",
    "    axs[2].imshow(combined_mask_da.values, cmap=\"gray\")\n",
    "    axs[2].set_title(\"combined_mask (1=masked)\")\n",
    "else:\n",
    "    axs[2].text(0.5,0.5,\"combined_mask_da missing\", ha=\"center\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D: Compare predictions at training points; show how many invasive training points were masked or mispredicted.\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# We need gdf_pts (reprojected training points) and arr -> sample the predicted layer at training locations\n",
    "if \"gdf\" not in globals():\n",
    "    print(\"gdf (training GeoDataFrame) not found\")\n",
    "else:\n",
    "    # ensure points in same CRS as raster\n",
    "    try:\n",
    "        gdf_pts = gdf.to_crs(median.odc.geobox.crs)\n",
    "    except Exception:\n",
    "        gdf_pts = gdf\n",
    "\n",
    "    gx = gdf_pts.geometry.x.values\n",
    "    gy = gdf_pts.geometry.y.values\n",
    "\n",
    "    # sample predicted_da at points (nearest)\n",
    "    if \"predicted_da\" not in globals():\n",
    "        print(\"predicted_da not found (run prediction step)\")\n",
    "    else:\n",
    "        # vectorized sampling\n",
    "        import xarray as xr\n",
    "        pts = \"pts\"\n",
    "        gx_da = xr.DataArray(gx, dims=pts)\n",
    "        gy_da = xr.DataArray(gy, dims=pts)\n",
    "        sampled_pred = predicted_da.sel({predicted_da.dims[-2]: gy_da, predicted_da.dims[-1]: gx_da}, method=\"nearest\").values\n",
    "        # sample mask at points\n",
    "        if \"combined_mask_da\" in globals():\n",
    "            sampled_mask = combined_mask_da.sel({combined_mask_da.dims[-2]: gy_da, combined_mask_da.dims[-1]: gx_da}, method=\"nearest\").values.astype(bool)\n",
    "        else:\n",
    "            sampled_mask = np.zeros_like(sampled_pred, dtype=bool)\n",
    "\n",
    "        # get training labels from gdf (column name assumed 'randomforest' — adjust if different)\n",
    "        lab_col = \"randomforest\"\n",
    "        if lab_col not in gdf_pts.columns:\n",
    "            print(f\"Training label column '{lab_col}' not found in gdf. Columns: {list(gdf_pts.columns)}\")\n",
    "        else:\n",
    "            y_true = gdf_pts[lab_col].values\n",
    "            # ensure numeric encoding similar to model if necessary (if y_true are strings and model predicted numeric labels, you might need to map)\n",
    "            print(\"Unique training labels:\", np.unique(y_true))\n",
    "            print(\"Unique predictions at training points (nan shown for missing):\", np.unique(sampled_pred[~np.isnan(sampled_pred)]))\n",
    "\n",
    "            # Show counts of training invasive points masked\n",
    "            # Find which label correspond to 'invasive' — we need the class value used in training. If training used numeric codes, report per-class\n",
    "            # We'll show a table where we print label, how many are masked, and how many predicted as same label\n",
    "            import pandas as pd\n",
    "            dfpts = pd.DataFrame({\n",
    "                \"true\": y_true,\n",
    "                \"pred\": sampled_pred,\n",
    "                \"masked\": sampled_mask\n",
    "            })\n",
    "            summary = dfpts.groupby(\"true\").agg(total=(\"true\",\"size\"), masked=(\"masked\",\"sum\"),\n",
    "                                               predicted_same=(\"pred\", lambda s: np.sum(s == dfpts.loc[s.index,\"true\"])))\n",
    "            print(\"\\nPer-class summary at training point locations:\")\n",
    "            print(summary)\n",
    "\n",
    "            # Confusion matrix for training points where prediction exists\n",
    "            valid_idx = ~np.isnan(sampled_pred)\n",
    "            if valid_idx.sum() > 0:\n",
    "                try:\n",
    "                    print(\"\\nClassification report (at training points):\")\n",
    "                    print(classification_report(y_true[valid_idx], sampled_pred[valid_idx]))\n",
    "                except Exception as e:\n",
    "                    print(\"Could not print classification report (maybe labels mismatch); here's confusion matrix:\")\n",
    "                    print(confusion_matrix(y_true[valid_idx], sampled_pred[valid_idx]))\n",
    "            else:\n",
    "                print(\"No valid predictions at training points (all NaN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell E: Sensitivity sweep across candidate thresholds to see masked area %\n",
    "import numpy as np\n",
    "\n",
    "# compute ndwi/ndbi/ndbai arrays if not in workspace\n",
    "ndwi = (median[\"green\"] - median[\"nir08\"]) / (median[\"green\"] + median[\"nir08\"])\n",
    "ndbi = (median[\"swir16\"] - median[\"nir08\"]) / (median[\"swir16\"] + median[\"nir08\"])\n",
    "ndbai = (median[\"swir16\"] - median[\"red\"]) / (median[\"swir16\"] + median[\"red\"])\n",
    "\n",
    "y_dim, x_dim = ndwi.dims[-2], ndwi.dims[-1]\n",
    "ndbi_vals = ndbi.values.flatten()\n",
    "ndbi_vals = ndbi_vals[np.isfinite(ndbi_vals)]\n",
    "\n",
    "# Sweeps\n",
    "ndbi_thr_candidates = np.linspace(np.percentile(ndbi_vals,5), np.percentile(ndbi_vals,95), 9)\n",
    "ndwi_thr_candidates = [0.1, 0.15, 0.2, 0.25]\n",
    "ndbai_thr_candidates = [0.1, 0.15, 0.2]\n",
    "\n",
    "print(\"Testing combinations (ndbi, ndwi, ndbai) -> %masked\")\n",
    "for ndbi_thr in ndbi_thr_candidates:\n",
    "    for ndwi_thr in ndwi_thr_candidates:\n",
    "        for ndbai_thr in ndbai_thr_candidates:\n",
    "            water_mask = (ndwi > ndwi_thr)\n",
    "            building_mask = (ndbi > ndbi_thr)\n",
    "            road_mask = (ndbai > ndbai_thr) & (ndwi < 0) & (ndbi < 0.2)\n",
    "            combined = (water_mask | building_mask | road_mask).values.astype(bool)\n",
    "            pct_masked = 100.0 * combined.sum() / combined.size\n",
    "            print(f\"ndbi={ndbi_thr:.3f}, ndwi={ndwi_thr:.2f}, ndbai={ndbai_thr:.2f} -> masked={pct_masked:.2f}%\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell F: Quick recipes to try — pick one and rerun the prediction cell afterwards\n",
    "\n",
    "# Option 1: Mask only water (keep settlements and roads unmasked)\n",
    "mask_only_water = ( ( (median[\"green\"] - median[\"nir08\"]) / (median[\"green\"] + median[\"nir08\"]) ) > 0.2 )\n",
    "mask_only_water = mask_only_water.astype(\"uint8\")\n",
    "print(\"Mask only water: masked% =\", 100*mask_only_water.values.sum()/mask_only_water.size)\n",
    "\n",
    "# Option 2: Reduce NDBI threshold (try 0.05 instead of 0.1)\n",
    "ndbi_thr_try = 0.05\n",
    "building_mask_lo = ( (median[\"swir16\"] - median[\"nir08\"]) / (median[\"swir16\"] + median[\"nir08\"]) ) > ndbi_thr_try\n",
    "combined_try = (mask_only_water | building_mask_lo).astype(\"uint8\")\n",
    "print(\"Mask water + building (ndbi 0.05) masked% =\", 100*combined_try.values.sum()/combined_try.size)\n",
    "\n",
    "# Option 3: Reduce morphological dilation (if you used dilation iterations>1)\n",
    "# If you used bm_np dilation iterations=1 earlier, try removing dilation or set iterations=0.\n",
    "\n",
    "# Option 4: Use mask as a predictor instead of excluding pixels\n",
    "print(\"To use the mask as a predictor, set INCLUDE_MASK_AS_FEATURE = True and re-run training/prediction cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Quick fix — mask only water so settlements remain visible\n",
    "# Run this after you have median and predicted_da in memory.\n",
    "\n",
    "ndwi = (median[\"green\"] - median[\"nir08\"]) / (median[\"green\"] + median[\"nir08\"])\n",
    "mask_only_water = (ndwi > 0.2).astype(\"uint8\")   # 0=keep, 1=water(mask)\n",
    "mask_only_water.name = \"mask_only_water\"\n",
    "print(\"Mask-only-water percent masked: {:.4f}%\".format(100.0 * mask_only_water.values.sum() / mask_only_water.size))\n",
    "\n",
    "# Apply to predicted layer (no retrain needed)\n",
    "predicted_masked_water = predicted_da.where(~mask_only_water.astype(bool), other=np.nan)\n",
    "predicted_masked_water.name = \"predicted_masked_water\"\n",
    "\n",
    "# Quick counts\n",
    "import numpy as np\n",
    "vals, counts = np.unique(np.nan_to_num(predicted_masked_water.values, nan=-9999), return_counts=True)\n",
    "print(\"Unique values in predicted_masked_water (nan->-9999):\")\n",
    "for v,c in zip(vals,counts):\n",
    "    print(v, c)\n",
    "\n",
    "# Optionally show a quick plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Predicted (masked only water)\")\n",
    "plt.imshow(np.nan_to_num(predicted_masked_water.values, nan=-1))\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust sampling of mask at training points and building summary DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ensure gdf_pts exists and is in raster CRS\n",
    "gdf_pts = gdf.to_crs(median.odc.geobox.crs)\n",
    "gx = gdf_pts.geometry.x.values\n",
    "gy = gdf_pts.geometry.y.values\n",
    "n_points = len(gx)\n",
    "print(\"n_points:\", n_points)\n",
    "\n",
    "# Use DataArray indexers so sel returns a 1D 'points' result when possible\n",
    "points_dim = \"points\"\n",
    "gx_da = xr.DataArray(gx, dims=points_dim)\n",
    "gy_da = xr.DataArray(gy, dims=points_dim)\n",
    "\n",
    "# Select mask values at point locations\n",
    "mask_sel = combined_mask_try.sel({combined_mask_try.dims[-2]: gy_da,\n",
    "                                  combined_mask_try.dims[-1]: gx_da},\n",
    "                                 method=\"nearest\")\n",
    "print(\"mask_sel.dims:\", mask_sel.dims, \"mask_sel.shape:\", mask_sel.shape)\n",
    "\n",
    "# Squeeze any singleton dims, then get numpy array and flatten to 1D\n",
    "mask_at_pts = mask_sel.squeeze(drop=True).values\n",
    "\n",
    "# If mask_at_pts is multi-dimensional (unexpected), try to reduce it sensibly:\n",
    "if mask_at_pts.ndim > 1:\n",
    "    # If one of the dims equals number of points, try to pick that axis\n",
    "    shapes = mask_at_pts.shape\n",
    "    print(\"mask_at_pts.ndim > 1; shapes:\", shapes)\n",
    "    # find an axis equal to n_points and move it to front then ravel\n",
    "    axis_with_points = None\n",
    "    for i, s in enumerate(shapes):\n",
    "        if s == n_points:\n",
    "            axis_with_points = i\n",
    "            break\n",
    "    if axis_with_points is not None:\n",
    "        # move that axis to 0 and then ravel other dims, keep one value per point by taking first along other axes\n",
    "        # e.g., if shape is (n_points,1) or (1,n_points), this will work\n",
    "        mask_at_pts = np.moveaxis(mask_at_pts, axis_with_points, 0)\n",
    "        # if remaining dims exist, collapse them by taking the first element along each remaining axis\n",
    "        while mask_at_pts.ndim > 1:\n",
    "            mask_at_pts = mask_at_pts[:, 0]\n",
    "        mask_at_pts = mask_at_pts.ravel()\n",
    "    else:\n",
    "        # last resort: flatten to 1D but warn (this likely misaligns points)\n",
    "        mask_at_pts = mask_at_pts.ravel()\n",
    "        print(\"Warning: mask_at_pts had no axis matching n_points; flattened array length:\", mask_at_pts.size)\n",
    "\n",
    "# Ensure mask_at_pts is 1-D and length matches number of points\n",
    "mask_at_pts = np.asarray(mask_at_pts).ravel()\n",
    "print(\"Final mask_at_pts.shape:\", mask_at_pts.shape)\n",
    "\n",
    "if mask_at_pts.shape[0] != n_points:\n",
    "    raise ValueError(f\"mask_at_pts length ({mask_at_pts.shape[0]}) does not match number of points ({n_points}). \"\n",
    "                     \"Check the selection dims and that combined_mask_try has spatial dims (y,x) compatible with median.\")\n",
    "\n",
    "# Build DataFrame safely\n",
    "dfpts = pd.DataFrame({\"true\": gdf_pts[\"randomforest\"].values, \"masked\": mask_at_pts.astype(bool)})\n",
    "summary = dfpts.groupby(\"true\").agg(total=(\"true\", \"size\"), masked=(\"masked\", \"sum\"))\n",
    "print(\"Per-class training-point masking summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: INCLUDE mask as a predictor and retrain the RandomForest (do this if you want model to learn mask info instead of excluding pixels)\n",
    "# Note: this re-runs training and prediction in-memory; it assumes you still have training_df extraction code available.\n",
    "# Steps: add combined_mask to median, re-extract training_df and observations, retrain clf, predict.\n",
    "\n",
    "# 1) use the combined_mask_try computed above (or combined_mask_da that you like). Convert to int8 and attach.\n",
    "median_with_mask = median.assign(combined_mask = combined_mask_try.astype(\"int8\"))\n",
    "\n",
    "# 2) Re-extract training samples (vectorized sampling) — the same code you used in Cell 8 earlier:\n",
    "arr = median_with_mask.to_array()  # dims: variable, y, x\n",
    "points_dim = \"points\"\n",
    "gx_da = xr.DataArray(gx, dims=points_dim)\n",
    "gy_da = xr.DataArray(gy, dims=points_dim)\n",
    "sampled = arr.sel({arr.dims[-2]: gy_da, arr.dims[-1]: gx_da}, method=\"nearest\").squeeze(drop=True)\n",
    "if sampled.ndim == 2:\n",
    "    sampled_df = sampled.transpose(points_dim, \"variable\").to_pandas()\n",
    "else:\n",
    "    # fallback nearest-index loop (should be rare)\n",
    "    y_coords = arr.coords[arr.dims[-2]].values\n",
    "    x_coords = arr.coords[arr.dims[-1]].values\n",
    "    iy = np.array([np.abs(y_coords - yy).argmin() for yy in gy])\n",
    "    ix = np.array([np.abs(x_coords - xx).argmin() for xx in gx])\n",
    "    vals = np.stack([arr.isel({arr.dims[-2]: iy_k, arr.dims[-1]: ix_k}).values.ravel()\n",
    "                     for iy_k, ix_k in zip(iy, ix)], axis=0)\n",
    "    var_names = list(arr.coords[\"variable\"].values)\n",
    "    sampled_df = pd.DataFrame(vals, columns=var_names)\n",
    "\n",
    "# Combine labels & drop NaNs\n",
    "training_df2 = pd.concat([gdf_pts.reset_index(drop=True)[\"randomforest\"], pd.DataFrame(sampled_df).reset_index(drop=True)], axis=1)\n",
    "training_df2 = training_df2.dropna()\n",
    "print(\"Training rows available for retrain (mask as feature):\", len(training_df2))\n",
    "\n",
    "# 3) Train RF again (quick)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier(n_estimators=RF_N_ESTIMATORS, random_state=RF_RANDOM_STATE, n_jobs=-1)\n",
    "y_train = training_df2.iloc[:,0].values\n",
    "X_train = training_df2.iloc[:,1:].values\n",
    "clf2.fit(X_train, y_train)\n",
    "print(\"Retrained RF with mask as feature\")\n",
    "\n",
    "# 4) Predict across the image (same stacking approach as before, but using median_with_mask)\n",
    "arr_full = median_with_mask.to_array().stack(pixels=(arr.dims[-2], arr.dims[-1])).transpose(\"pixels\",\"variable\")\n",
    "X_full = arr_full.values\n",
    "valid_mask_full = ~np.any(np.isnan(X_full), axis=1)\n",
    "pred_flat2 = np.full(X_full.shape[0], np.nan, dtype=np.float32)\n",
    "if valid_mask_full.sum() > 0:\n",
    "    pred_flat2[valid_mask_full] = clf2.predict(X_full[valid_mask_full]).astype(np.float32)\n",
    "pred_2d_2 = pred_flat2.reshape(median[arr.dims[-2]].size, median[arr.dims[-1]].size)\n",
    "predicted_da_with_mask_feature = xr.DataArray(pred_2d_2, coords={arr.dims[-2]: median[arr.dims[-2]], arr.dims[-1]: median[arr.dims[-1]]}, dims=(arr.dims[-2], arr.dims[-1]))\n",
    "predicted_da_with_mask_feature.name = \"predicted_with_mask_feature\"\n",
    "print(\"Predicted with mask included as feature. Unique values:\", np.unique(predicted_da_with_mask_feature.values[~np.isnan(predicted_da_with_mask_feature.values)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust: sample combined_mask_try at training points and build per-class summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ensure gdf_pts exists and is in raster CRS\n",
    "try:\n",
    "    gdf_pts\n",
    "except NameError:\n",
    "    gdf_pts = gdf.to_crs(median.odc.geobox.crs)\n",
    "\n",
    "gx = gdf_pts.geometry.x.values\n",
    "gy = gdf_pts.geometry.y.values\n",
    "n_points = len(gx)\n",
    "print(\"n_points:\", n_points)\n",
    "\n",
    "# Inspect mask dims for diagnostics\n",
    "print(\"combined_mask_try dims:\", combined_mask_try.dims, \"shape:\", combined_mask_try.shape)\n",
    "\n",
    "# Vectorized selection with a 'points' DataArray indexer\n",
    "points_dim = \"points\"\n",
    "gx_da = xr.DataArray(gx, dims=points_dim)\n",
    "gy_da = xr.DataArray(gy, dims=points_dim)\n",
    "\n",
    "mask_sel = combined_mask_try.sel({combined_mask_try.dims[-2]: gy_da,\n",
    "                                  combined_mask_try.dims[-1]: gx_da},\n",
    "                                 method=\"nearest\")\n",
    "print(\"mask_sel.dims:\", mask_sel.dims, \"mask_sel.shape:\", mask_sel.shape)\n",
    "\n",
    "# Squeeze singleton dims\n",
    "mask_vals = mask_sel.squeeze(drop=True).values\n",
    "print(\"After squeeze, mask_vals.shape:\", getattr(mask_vals, \"shape\", None), \"ndim:\", getattr(mask_vals, \"ndim\", None))\n",
    "\n",
    "# If mask_vals still has >1 dim, try to find the axis that corresponds to points\n",
    "if np.ndim(mask_vals) > 1:\n",
    "    shapes = mask_vals.shape\n",
    "    print(\"mask_vals multi-dim shapes:\", shapes)\n",
    "    # find axis equal to n_points\n",
    "    axis_with_points = next((i for i,s in enumerate(shapes) if s == n_points), None)\n",
    "    if axis_with_points is not None:\n",
    "        # move that axis to front and collapse remaining axes by taking first element along them\n",
    "        mask_vals = np.moveaxis(mask_vals, axis_with_points, 0)\n",
    "        while mask_vals.ndim > 1:\n",
    "            mask_vals = mask_vals[:, 0]\n",
    "        mask_vals = mask_vals.ravel()\n",
    "        print(\"Reduced mask_vals via axis_with_points -> shape:\", mask_vals.shape)\n",
    "    else:\n",
    "        # as last resort, try to flatten but warn if lengths won't match\n",
    "        mask_vals = mask_vals.ravel()\n",
    "        print(\"Warning: no axis matches n_points; flattened mask_vals length:\", mask_vals.size)\n",
    "\n",
    "# Ensure final array is 1-D and matches points\n",
    "mask_at_pts = np.asarray(mask_vals).ravel()\n",
    "print(\"Final mask_at_pts.shape:\", mask_at_pts.shape)\n",
    "\n",
    "if mask_at_pts.shape[0] != n_points:\n",
    "    raise ValueError(f\"mask_at_pts length ({mask_at_pts.shape[0]}) != number of points ({n_points}). \"\n",
    "                     \"Check combined_mask_try dims and that it is spatial (y,x). See printed diagnostics above.\")\n",
    "\n",
    "# Now build DataFrame and summary safely\n",
    "dfpts = pd.DataFrame({\"true\": gdf_pts[\"randomforest\"].values, \"masked\": mask_at_pts.astype(bool)})\n",
    "summary = dfpts.groupby(\"true\").agg(total=(\"true\", \"size\"), masked=(\"masked\", \"sum\"))\n",
    "print(\"Per-class training-point masking summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predicted_da:\", getattr(predicted_da, \"dims\", None), predicted_da.shape)\n",
    "print(\"predicted_masked:\", getattr(predicted_masked, \"dims\", None), getattr(predicted_masked, \"shape\", None))\n",
    "print(\"median coords:\", list(median.coords))\n",
    "print(\"CRS (odc geobox):\", median.odc.geobox.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# RGB median\n",
    "fig, axs = plt.subplots(1,3, figsize=(18,6))\n",
    "if all(b in median for b in (\"red\",\"green\",\"blue\")):\n",
    "    rgb = np.dstack([median[\"red\"].values, median[\"green\"].values, median[\"blue\"].values])\n",
    "    rgb = np.clip(rgb, 0, 1)  # ensure range 0-1\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"Median RGB\")\n",
    "else:\n",
    "    axs[0].text(0.5,0.5,\"RGB not available\", ha=\"center\")\n",
    "\n",
    "# Predicted (raw)\n",
    "im = axs[1].imshow(np.nan_to_num(predicted_da.values, nan=-1), cmap=\"viridis\")\n",
    "axs[1].set_title(\"Predicted (raw classes)\")\n",
    "plt.colorbar(im, ax=axs[1], fraction=0.046)\n",
    "\n",
    "# Masked predicted (invasives visible only where not masked)\n",
    "im2 = axs[2].imshow(np.nan_to_num(predicted_masked.values, nan=-1), cmap=\"tab20\")\n",
    "axs[2].set_title(\"Predicted (masked)\")\n",
    "plt.colorbar(im2, ax=axs[2], fraction=0.046)\n",
    "\n",
    "for ax in axs: ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicted_masked as GeoTIFF using rioxarray\n",
    "import rioxarray\n",
    "# ensure spatial metadata exists (ODC geobox provides it)\n",
    "da = predicted_masked  # xarray.DataArray (y,x)\n",
    "# write CRS info (use median.odc.geobox.crs)\n",
    "da = da.rio.write_crs(median.odc.geobox.crs, inplace=False)\n",
    "da.rio.to_raster(\"predicted_masked.tif\", compress=\"LZW\")\n",
    "print(\"Saved predicted_masked.tif\")\n",
    "# save combined mask\n",
    "combined_mask_da.rio.write_crs(median.odc.geobox.crs, inplace=True)\n",
    "combined_mask_da.rio.to_raster(\"combined_mask.tif\", compress=\"LZW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
