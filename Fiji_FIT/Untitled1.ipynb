{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports, configuration, and small helpers\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.transform import resize\n",
    "import joblib\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optional: rioxarray for saving GeoTIFF\n",
    "try:\n",
    "    import rioxarray  # noqa: F401\n",
    "    RIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    RIO_AVAILABLE = False\n",
    "\n",
    "# User-configurable values - edit these\n",
    "STAC_CATALOG = \"https://earth-search.aws.element84.com/v1\"   # STAC endpoint\n",
    "AOI_PATH = \"AOI/EfateAOI.geojson\"                           # path to your AOI GeoJSON\n",
    "TRAINING_PATH = \"Training_Data/InvasiveClean6.geojson\"      # training points GeoJSON\n",
    "DATETIME = \"2024-05/2024-09\"                                # date range for search\n",
    "SENTINEL_COLLECTION = [\"sentinel-2-c1-l2a\"]\n",
    "\n",
    "# Mask/band options\n",
    "MASK_NAME = \"combined_mask\"\n",
    "INCLUDE_MASK_AS_FEATURE = False   # True to include mask as a predictor band, False to only use to exclude pixels\n",
    "\n",
    "# Model / outputs\n",
    "RF_N_ESTIMATORS = 100\n",
    "RF_RANDOM_STATE = 42\n",
    "MODEL_OUT = \"rf_model.joblib\"\n",
    "PRED_GTIFF = \"predicted.tif\"\n",
    "OUT_DS_NETCDF = \"predicted_dataset.nc\"\n",
    "\n",
    "# Small helper for debugging prints\n",
    "def info(msg, *args):\n",
    "    print(\"[INFO]\", msg, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: STAC client, AOI load, and Dask local client\n",
    "from pystac_client import Client\n",
    "from dask.distributed import Client as DaskClient\n",
    "from odc.stac import load, configure_s3_access\n",
    "\n",
    "info(\"Opening STAC catalog:\", STAC_CATALOG)\n",
    "catalog = Client.open(STAC_CATALOG)\n",
    "\n",
    "# Load AOI\n",
    "aoi_gdf = gpd.read_file(AOI_PATH)\n",
    "info(\"AOI bounds:\", aoi_gdf.total_bounds)\n",
    "aoi_gdf.plot(edgecolor=\"red\", facecolor=\"none\")\n",
    "plt.title(\"AOI Check\")\n",
    "plt.show()\n",
    "\n",
    "bbox = aoi_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Start a local dask client (adjust workers/threads/memory as needed)\n",
    "dask_client = DaskClient(n_workers=1, threads_per_worker=16, memory_limit=\"16GB\")\n",
    "info(\"Dask client started:\", dask_client)\n",
    "# Configure S3 access for ODC\n",
    "configure_s3_access(cloud_defaults=True, requester_pays=True)\n",
    "info(\"Configured S3 access (cloud_defaults=True, requester_pays=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load training points and inspect\n",
    "gdf = gpd.read_file(TRAINING_PATH, bbox=tuple(bbox))\n",
    "info(\"Training points loaded:\", len(gdf), \"records\")\n",
    "# Quick interactive check (in notebook)\n",
    "gdf.explore(column=\"randomforest\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: STAC search and load into an xarray.Dataset\n",
    "items = catalog.search(\n",
    "    collections=SENTINEL_COLLECTION,\n",
    "    bbox=bbox,\n",
    "    datetime=DATETIME,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 25}},\n",
    ").item_collection()\n",
    "\n",
    "info(\"Found items:\", len(items))\n",
    "# Load relevant measurements into an xarray.Dataset; chunk sizes tuned to your environment\n",
    "data = load(\n",
    "    items,\n",
    "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"scl\"],\n",
    "    bbox=bbox,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "info(\"Loaded data variables:\", list(data.data_vars))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Cloud mask, scaling, indices, median composite\n",
    "# SCL mask values to exclude: 1 (defective), 3 (shadow), 9 (cloud), 10 (thin cirrus)\n",
    "mask_flags = [1, 3, 9, 10]\n",
    "cloud_mask = ~data.scl.isin(mask_flags)\n",
    "masked = data.where(cloud_mask)\n",
    "\n",
    "# scale to 0-1 and clip (Sentinel-2 L2A uses 10000 scaling)\n",
    "scaled = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "# add NDVI as an example (keeps coords/dims)\n",
    "scaled[\"ndvi\"] = (scaled.nir08 - scaled.red) / (scaled.nir08 + scaled.red)\n",
    "\n",
    "info(\"Computing median composite (this may take a few minutes)...\")\n",
    "median = scaled.median(\"time\").compute()\n",
    "info(\"Median composite computed. Variables:\", list(median.data_vars))\n",
    "\n",
    "# Quick check\n",
    "median.odc.explore(vmin=0, vmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Compute combined mask (water, built-up, bare/roads) from the median composite\n",
    "# Use the bands in the median dataset; adjust names if your dataset uses different variable names\n",
    "green = median[\"green\"]\n",
    "red = median[\"red\"]\n",
    "nir = median[\"nir08\"]\n",
    "swir = median[\"swir16\"]\n",
    "\n",
    "# Indices (xarray operations retain coords)\n",
    "ndwi  = (green - nir) / (green + nir)\n",
    "ndbi  = (swir - nir) / (swir + nir)\n",
    "ndbai = (swir - red) / (swir + red)\n",
    "\n",
    "water_mask    = ndwi > 0.2\n",
    "building_mask = ndbi > 0.1\n",
    "road_mask     = (ndbai > 0.15) & (ndwi < 0) & (ndbi < 0.2)\n",
    "\n",
    "combined_mask_da = (water_mask | building_mask | road_mask).astype(\"uint8\")\n",
    "combined_mask_da.name = MASK_NAME\n",
    "info(\"Combined mask created with dims:\", combined_mask_da.dims)\n",
    "\n",
    "# Show the mask quick preview\n",
    "try:\n",
    "    combined_mask_da.odc.to_rgba(palette=[\"none\",\"red\"], alpha=0.3).odc.explore()  # interactive\n",
    "except Exception:\n",
    "    print(\"Preview: mask min/max:\", float(combined_mask_da.min()), float(combined_mask_da.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Add mask as band if desired, otherwise keep it separate for filtering\n",
    "if INCLUDE_MASK_AS_FEATURE:\n",
    "    # add mask into median dataset so to_array includes it as a feature\n",
    "    median = median.assign(**{MASK_NAME: combined_mask_da})\n",
    "    info(\"Mask added to median dataset as a band. Variables now:\", list(median.data_vars))\n",
    "else:\n",
    "    mask_da = combined_mask_da  # we'll use mask_da for filtering\n",
    "    info(\"Mask kept separate; will be used to exclude pixels from training/prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 (replacement): Prepare training samples - reproject, sample raster values at point locations, filter masked points\n",
    "\n",
    "# Reproject training points to raster CRS\n",
    "gdf_pts = gdf.to_crs(median.odc.geobox.crs)\n",
    "gx = gdf_pts.geometry.x.values\n",
    "gy = gdf_pts.geometry.y.values\n",
    "\n",
    "# Convert median (Dataset) to a DataArray of bands for sampling\n",
    "arr = median.to_array()  # expected dims: ('variable','y','x') but inspect below\n",
    "print(\"arr.dims:\", arr.dims)\n",
    "print(arr)  # inspect structure if something unexpected appears\n",
    "\n",
    "# Identify spatial dims (assume last two dims are spatial)\n",
    "y_dim, x_dim = arr.dims[-2], arr.dims[-1]\n",
    "points_dim = \"points\"\n",
    "\n",
    "# Vectorized sampling using DataArray indexers (fast)\n",
    "gx_da = xr.DataArray(gx, dims=points_dim)\n",
    "gy_da = xr.DataArray(gy, dims=points_dim)\n",
    "\n",
    "sampled = arr.sel({y_dim: gy_da, x_dim: gx_da}, method=\"nearest\")\n",
    "print(\"sampled.dims (after sel):\", sampled.dims)\n",
    "\n",
    "# Remove any singleton dims that might have been created\n",
    "sampled = sampled.squeeze(drop=True)\n",
    "print(\"sampled.dims (after squeeze):\", sampled.dims, \"ndim:\", sampled.ndim)\n",
    "\n",
    "# If we now have a 2D DataArray (variable, points) we can convert to pandas, otherwise fallback\n",
    "if sampled.ndim == 2:\n",
    "    # transpose so rows=points, cols=variables and convert to pandas\n",
    "    sampled_df = sampled.transpose(points_dim, \"variable\").to_pandas()\n",
    "    sampled_df = pd.DataFrame(sampled_df).reset_index(drop=True)\n",
    "else:\n",
    "    # Fallback: explicit nearest-index lookup per point (slower but robust)\n",
    "    import numpy as np\n",
    "    print(\"Vectorized sel did not produce 2D output. Falling back to explicit nearest-index lookup...\")\n",
    "    y_coords = arr.coords[y_dim].values\n",
    "    x_coords = arr.coords[x_dim].values\n",
    "\n",
    "    # compute index of nearest raster cell for each point\n",
    "    iy = np.array([np.abs(y_coords - yy).argmin() for yy in gy])\n",
    "    ix = np.array([np.abs(x_coords - xx).argmin() for xx in gx])\n",
    "\n",
    "    # collect values: for each point extract arr[:, iy[k], ix[k]] -> 1D (variable)\n",
    "    vals = np.stack([\n",
    "        arr.isel({y_dim: iy_k, x_dim: ix_k}).values.ravel()\n",
    "        for iy_k, ix_k in zip(iy, ix)\n",
    "    ], axis=0)   # shape (n_points, n_variables)\n",
    "\n",
    "    # variable names (arr.coords['variable'] exists because we used to_array())\n",
    "    var_names = list(arr.coords[\"variable\"].values)\n",
    "    sampled_df = pd.DataFrame(vals, columns=var_names).reset_index(drop=True)\n",
    "\n",
    "# Combine with class labels\n",
    "training_df = pd.concat([gdf_pts.reset_index(drop=True)[\"randomforest\"], sampled_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Optionally filter out points that fall into combined_mask==1 (if not using mask as feature)\n",
    "if not INCLUDE_MASK_AS_FEATURE:\n",
    "    mask_at_pts = combined_mask_da.sel({combined_mask_da.dims[-2]: gy, combined_mask_da.dims[-1]: gx}, method=\"nearest\").values\n",
    "    keep = (mask_at_pts == 0)\n",
    "    training_df = training_df.loc[keep].reset_index(drop=True)\n",
    "    info(f\"Kept {training_df.shape[0]} training points after mask filtering (out of {len(gx)})\")\n",
    "\n",
    "# Drop NaNs and preview\n",
    "training_df = training_df.dropna()\n",
    "info(\"Training rows used for model fitting after dropna:\", len(training_df))\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train RandomForest classifier\n",
    "classes = training_df.iloc[:, 0].values\n",
    "observations = training_df.iloc[:, 1:].values\n",
    "info(\"Observation shape:\", observations.shape)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=RF_N_ESTIMATORS, random_state=RF_RANDOM_STATE, n_jobs=-1)\n",
    "clf.fit(observations, classes)\n",
    "info(\"RandomForest fitted. Estimators:\", RF_N_ESTIMATORS)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(clf, MODEL_OUT)\n",
    "info(\"Saved model to\", MODEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Predict full image\n",
    "# Stack median into (pixels, variables)\n",
    "arr = median.to_array()  # dims: variable, y, x\n",
    "# Determine spatial dim names from arr\n",
    "y_dim, x_dim = arr.dims[-2], arr.dims[-1]\n",
    "ny, nx = arr.sizes[y_dim], arr.sizes[x_dim]\n",
    "\n",
    "stacked = arr.stack(pixels=(y_dim, x_dim)).transpose(\"pixels\", \"variable\")  # pixels x variables\n",
    "X = stacked.values  # numpy array shape (n_pixels, n_bands)\n",
    "info(\"Full predictor array shape:\", X.shape)\n",
    "\n",
    "# Build valid mask: no NaNs in features\n",
    "valid_data_mask = ~np.any(np.isnan(X), axis=1)\n",
    "\n",
    "if not INCLUDE_MASK_AS_FEATURE:\n",
    "    mask_flat = combined_mask_da.stack(pixels=(y_dim, x_dim)).values.astype(bool)\n",
    "    predict_mask = valid_data_mask & (~mask_flat)\n",
    "else:\n",
    "    predict_mask = valid_data_mask\n",
    "\n",
    "info(\"Pixels to predict:\", predict_mask.sum(), \"out of\", len(predict_mask))\n",
    "\n",
    "# Predict only valid pixels\n",
    "pred_flat = np.full(X.shape[0], np.nan, dtype=np.float32)\n",
    "if predict_mask.sum() > 0:\n",
    "    pred_flat[predict_mask] = clf.predict(X[predict_mask]).astype(np.float32)\n",
    "\n",
    "pred_2d = pred_flat.reshape(ny, nx)\n",
    "predicted_da = xr.DataArray(pred_2d, coords={y_dim: median[y_dim], x_dim: median[x_dim]}, dims=(y_dim, x_dim))\n",
    "predicted_da.name = \"predicted_class\"\n",
    "info(\"Prediction completed. Dtype:\", predicted_da.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Build output Dataset and save outputs (GeoTIFF / NetCDF)\n",
    "out_vars = {\"predicted\": predicted_da, MASK_NAME: combined_mask_da}\n",
    "\n",
    "out_ds = xr.Dataset(out_vars)\n",
    "info(\"Output dataset prepared. Variables:\", list(out_ds.data_vars))\n",
    "\n",
    "if RIO_AVAILABLE:\n",
    "    try:\n",
    "        # write CRS and to raster (GeoTIFF)\n",
    "        out_ds[\"predicted\"].rio.write_crs(median.odc.geobox.crs, inplace=True)\n",
    "        out_ds[\"predicted\"].rio.to_raster(PRED_GTIFF)\n",
    "        info(\"Saved predicted GeoTIFF to\", PRED_GTIFF)\n",
    "    except Exception as e:\n",
    "        info(\"Could not save GeoTIFF via rioxarray:\", e)\n",
    "        out_ds.to_netcdf(OUT_DS_NETCDF)\n",
    "        info(\"Saved NetCDF to\", OUT_DS_NETCDF)\n",
    "else:\n",
    "    out_ds.to_netcdf(OUT_DS_NETCDF)\n",
    "    info(\"rioxarray not available; saved NetCDF to\", OUT_DS_NETCDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Visualize using folium (interactive)\n",
    "# compute a map center (attempt using training points in lon/lat)\n",
    "try:\n",
    "    # gdf_pts currently in raster CRS; convert to geographic WGS84 if needed\n",
    "    gdf_wgs84 = gdf_pts.to_crs(epsg=4326)\n",
    "    center = [float(gdf_wgs84.geometry.y.mean()), float(gdf_wgs84.geometry.x.mean())]\n",
    "except Exception:\n",
    "    # fallback to AOI centroid (AOI likely in lon/lat already)\n",
    "    centroid = aoi_gdf.to_crs(epsg=4326).geometry.centroid.iloc[0]\n",
    "    center = [centroid.y, centroid.x]\n",
    "\n",
    "m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "# Add median RGB layer (if odc helpers available)\n",
    "try:\n",
    "    median.odc.to_rgba(vmin=0, vmax=0.3).odc.add_to(m, name=\"Median Composite\")\n",
    "except Exception:\n",
    "    info(\"ODC RGBA helper not available for median visualization\")\n",
    "\n",
    "# Add predicted classes (if odc helpers exist)\n",
    "try:\n",
    "    predicted_da.odc.add_to(m, name=\"Predicted\")\n",
    "except Exception:\n",
    "    info(\"ODC helper not available for predicted layer\")\n",
    "\n",
    "# Add combined mask as a semi-transparent layer (if supported)\n",
    "try:\n",
    "    combined_mask_da.astype(\"uint8\").odc.to_rgba(palette=[\"none\",\"red\"], alpha=0.3).odc.add_to(m, name=\"Combined Mask\")\n",
    "except Exception:\n",
    "    info(\"ODC helper not available for mask display\")\n",
    "\n",
    "# Add training points (convert to WGS84 for folium)\n",
    "try:\n",
    "    gdf_wgs84.explore(m=m, column=\"randomforest\", legend=True, name=\"Training Data\")\n",
    "except Exception:\n",
    "    info(\"Could not add training points layer to map\")\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: (Optional) Quick evaluation on training data (be careful: this is training accuracy)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate on training rows used\n",
    "train_preds = clf.predict(observations)\n",
    "print(\"Classification report (training data):\")\n",
    "print(classification_report(classes, train_preds))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(classes, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
